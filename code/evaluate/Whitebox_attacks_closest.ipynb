{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D,Conv2D,MaxPooling2D,Input,Lambda,GlobalMaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from matplotlib.pyplot import imread,imshow\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage.io import imsave\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist_emb_file = 'whitelist_emb.npy' #precomputed embeddings \n",
    "phish_emb_file = 'phishing_emb.npy' #precomputed embeddings \n",
    "output_dir = './'\n",
    "saved_model_name = 'model' \n",
    "\n",
    "# Dataset parameters \n",
    "dataset_path = '../../datasets/WhitePhish/'\n",
    "reshape_size = [224,224,3]\n",
    "num_targets = 155 \n",
    "\n",
    "#adv parameters\n",
    "epsilon = 0.005\n",
    "batch_size = 3\n",
    "trials = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_imgs_per_website(data_path,targets,imgs_num,reshape_size,start_target_count):\n",
    "    all_imgs = np.zeros(shape=[imgs_num,224,224,3])\n",
    "    all_labels = np.zeros(shape=[imgs_num,1])\n",
    "    \n",
    "    all_file_names = []\n",
    "    targets_list = targets.splitlines()\n",
    "    count = 0\n",
    "    for i in range(0,len(targets_list)):\n",
    "        target_path = data_path + targets_list[i]\n",
    "        print(target_path)\n",
    "        file_names = sorted(os.listdir(target_path))\n",
    "        for j in range(0,len(file_names)):\n",
    "            try:\n",
    "                img = imread(target_path+'/'+file_names[j])\n",
    "                img = img[:,:,0:3]\n",
    "                all_imgs[count,:,:,:] = resize(img, (reshape_size[0], reshape_size[1]),anti_aliasing=True)\n",
    "                all_labels[count,:] = i + start_target_count\n",
    "                all_file_names.append(file_names[j])\n",
    "                count = count + 1\n",
    "            except:\n",
    "                #some images were saved with a wrong extensions \n",
    "                try:\n",
    "                    img = imread(target_path+'/'+file_names[j],format='jpeg')\n",
    "                    img = img[:,:,0:3]\n",
    "                    all_imgs[count,:,:,:] = resize(img, (reshape_size[0], reshape_size[1]),anti_aliasing=True)\n",
    "                    all_labels[count,:] = i + start_target_count\n",
    "                    all_file_names.append(file_names[j])\n",
    "                    count = count + 1\n",
    "                except:\n",
    "                    print('failed at:')\n",
    "                    print('***')\n",
    "                    print(file_names[j])\n",
    "                    break \n",
    "    return all_imgs,all_labels,all_file_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images legit (train)\n",
    "data_path = dataset_path + 'legitimate_whitelist/'\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "imgs_num = 9363\n",
    "all_imgs_train,all_labels_train,all_file_names_train = read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "# Read images phishing\n",
    "data_path = dataset_path + 'phishing/'\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "imgs_num = 1195\n",
    "all_imgs_test,all_labels_test,all_file_names_test = read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_legit = all_imgs_train\n",
    "y_train_legit = all_labels_train\n",
    "\n",
    "\n",
    "phish_train_idx = np.load(output_dir+'train_idx.npy')\n",
    "phish_test_idx = np.load(output_dir+'test_idx.npy')\n",
    "\n",
    "\n",
    "X_train_phish = all_imgs_test[phish_train_idx,:]\n",
    "y_train_phish = all_labels_test[phish_train_idx,:]\n",
    "\n",
    "X_test_phish = all_imgs_test[phish_test_idx,:]\n",
    "y_test_phish = all_labels_test[phish_test_idx,:]\n",
    "y_test_phish_non_ordered = all_labels_test[phish_test_idx,:]\n",
    "\n",
    "\n",
    "def order_random_array(orig_arr,y_orig_arr,targets):\n",
    "    sorted_arr = np.zeros(orig_arr.shape)\n",
    "    y_sorted_arr = np.zeros(y_orig_arr.shape)\n",
    "    count = 0\n",
    "    for i in range(0,targets):\n",
    "        for j in range(0,orig_arr.shape[0]):\n",
    "            if y_orig_arr[j] == i:\n",
    "                sorted_arr[count,:,] = orig_arr[j,:]\n",
    "                y_sorted_arr[count,:] = i\n",
    "                count = count + 1\n",
    "    return sorted_arr,y_sorted_arr \n",
    "X_train_phish_features,y_train_phish_ordered = order_random_array(X_train_phish_features,y_train_phish,num_targets)\n",
    "\n",
    "X_test_phish,y_test_phish = order_random_array(X_test_phish,y_test_phish,num_targets)\n",
    "X_train_phish,y_train_phish = order_random_array(X_train_phish,y_train_phish,num_targets)\n",
    "\n",
    "\n",
    "#get start and end of each label\n",
    "def start_end_each_target_not_complete(num_target,labels):\n",
    "    prev_target = labels[0]\n",
    "    start_end_each_target = np.zeros((num_target,2))\n",
    "    start_end_each_target[0,0] = labels[0]\n",
    "    if not labels[0] == 0:\n",
    "        start_end_each_target[0,0] = -1\n",
    "        start_end_each_target[0,1] = -1\n",
    "    count_target = 0\n",
    "    for i in range(1,labels.shape[0]):\n",
    "        if not labels[i] == prev_target:\n",
    "            start_end_each_target[int(labels[i-1]),1] = int(i-1)\n",
    "            #count_target = count_target + 1\n",
    "            start_end_each_target[int(labels[i]),0] = int(i)\n",
    "            prev_target = labels[i]\n",
    "    start_end_each_target[int(labels[-1]),1] = int(labels.shape[0]-1)\n",
    "    \n",
    "    for i in range(1,num_target):\n",
    "        if start_end_each_target[i,0] == 0:\n",
    "            start_end_each_target[i,0] = -1\n",
    "            start_end_each_target[i,1] = -1\n",
    "    return start_end_each_target\n",
    "\n",
    "labels_start_end_train_phish = start_end_each_target_not_complete(num_targets,y_train_phish)\n",
    "labels_start_end_test_phish = start_end_each_target_not_complete(num_targets,y_test_phish)\n",
    "\n",
    "def start_end_each_target(num_target,labels):\n",
    "    prev_target = 0\n",
    "    start_end_each_target = np.zeros((num_target,2))\n",
    "    start_end_each_target[0,0] = 0\n",
    "    count_target = 0\n",
    "    for i in range(1,labels.shape[0]):\n",
    "        if not labels[i] == prev_target:\n",
    "            start_end_each_target[count_target,1] = i-1\n",
    "            count_target = count_target + 1\n",
    "            start_end_each_target[count_target,0] = i\n",
    "            prev_target = prev_target + 1\n",
    "    start_end_each_target[num_target-1,1] = labels.shape[0]-1\n",
    "    return start_end_each_target\n",
    "\n",
    "labels_start_end_train_legit = start_end_each_target(num_targets,y_train_legit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "margin = 2.2\n",
    "def loss(y_true,y_pred):\n",
    "    loss_value = K.maximum(y_true, margin + y_pred)\n",
    "    loss_value = K.mean(loss_value,axis=0)\n",
    "    return loss_value\n",
    "\n",
    "full_model = load_model(output_dir+saved_model_name+'.h5', custom_objects={'loss': loss})\n",
    "inside_model = full_model.layers[3]\n",
    "\n",
    "#define custom_loss\n",
    "def custom_loss(margin):\n",
    "    def loss(y_true,y_pred):\n",
    "        loss_value = K.maximum(y_true, margin + y_pred)\n",
    "        loss_value = K.mean(loss_value,axis=0)\n",
    "        return loss_value\n",
    "    return loss\n",
    "my_loss = custom_loss(60) #set margin to a large value in order to always have a non-zero loss in adv generation.\n",
    "\n",
    "#get tf session\n",
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_legit_features = np.load(output_dir+whitelist_emb_file)\n",
    "phish_features = np.load(output_dir+phish_emb_file)\n",
    "\n",
    "X_test_phish_features = phish_features[phish_test_idx,:]\n",
    "X_train_phish_features = phish_features[phish_train_idx,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_pos_img_idx(prob_phish,img_label):\n",
    "    if np.random.uniform() > prob_phish:\n",
    "        #take image from legit\n",
    "        class_idx_start_end = labels_start_end_train_legit[img_label,:]\n",
    "        same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "        img = X_train_legit[same_idx,:]\n",
    "    else:\n",
    "        #take from phish\n",
    "        if not labels_start_end_train_phish[img_label,0] == -1:\n",
    "            class_idx_start_end = labels_start_end_train_phish[img_label,:]\n",
    "            same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "            img = X_train_phish[same_idx,:]\n",
    "        else:\n",
    "            class_idx_start_end = labels_start_end_train_legit[img_label,:]\n",
    "            same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "            img = X_train_legit[same_idx,:]\n",
    "    return img\n",
    "\n",
    "\n",
    "def pick_neg_img(anchor_idx,num_targets):\n",
    "    if anchor_idx == 0:\n",
    "        targets = np.arange(1,num_targets)\n",
    "    elif anchor_idx == num_targets -1:\n",
    "        targets = np.arange(0,num_targets-1)\n",
    "    else:\n",
    "        targets = np.concatenate([np.arange(0,anchor_idx),np.arange(anchor_idx+1,num_targets)])\n",
    "    diff_target_idx = np.random.randint(low = 0,high = num_targets-1)\n",
    "    diff_target = targets[diff_target_idx]\n",
    "    \n",
    "    class_idx_start_end = labels_start_end_train_legit[diff_target,:]\n",
    "    idx_from_diff_target = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "    img = X_train_legit[idx_from_diff_target,:]\n",
    "    \n",
    "    return img,diff_target\n",
    "\n",
    "\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "all_targets = targets_file.read()\n",
    "all_targets = all_targets.splitlines()\n",
    "\n",
    "def get_idx_of_target(target_name,all_targets):\n",
    "    for i in range(0,len(all_targets)):\n",
    "        if all_targets[i] == target_name:\n",
    "            found_idx = i\n",
    "            return found_idx\n",
    "\n",
    "target_lists = [['microsoft','ms_outlook','ms_office','ms_bing','ms_onedrive','ms_skype'],['apple','itunes','icloud'],['google','google_drive'],['alibaba','aliexpress']]\n",
    "\n",
    "def get_associated_targets_idx(target_lists,all_targets):\n",
    "    sub_target_lists_idx = []\n",
    "    parents_ids = []\n",
    "    for i in range(0,len(target_lists)):\n",
    "        target_list = target_lists[i]\n",
    "        parent_target = target_list[0]\n",
    "        one_target_list = []\n",
    "        parent_idx = get_idx_of_target(parent_target,all_targets)\n",
    "        parents_ids.append(parent_idx)\n",
    "        for child_target in target_list[1:]:\n",
    "            child_idx = get_idx_of_target(child_target,all_targets)\n",
    "            one_target_list.append(child_idx)\n",
    "        sub_target_lists_idx.append(one_target_list)\n",
    "    return parents_ids,sub_target_lists_idx \n",
    "\n",
    "parents_ids,sub_target_lists_idx  = get_associated_targets_idx(target_lists,all_targets)\n",
    "\n",
    "def check_if_same_category(img_label1,img_label2):\n",
    "    if_same = 0\n",
    "    if img_label1 in parents_ids:\n",
    "        if img_label2 in sub_target_lists_idx[parents_ids.index(img_label1)]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[0]:\n",
    "        if img_label2 in sub_target_lists_idx[0] or img_label2 == parents_ids[0]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[1]:\n",
    "        if img_label2 in sub_target_lists_idx[1] or img_label2 == parents_ids[1]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[2]:\n",
    "        if img_label2 in sub_target_lists_idx[2] or img_label2 == parents_ids[2]:\n",
    "            if_same = 1\n",
    "    return if_same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find closest example to each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_pair(layer1,layer2):\n",
    "    diff = layer1 - layer2\n",
    "    l2_diff = np.mean(diff**2)\n",
    "    return l2_diff\n",
    "\n",
    "def argmax(lst):\n",
    "    return lst.index(max(lst))\n",
    "\n",
    "def argmin(lst):\n",
    "    return lst.index(min(lst))\n",
    "\n",
    "\n",
    "# Find closest example from the same website:\n",
    "# Assume ordered training arrays (legit,train).\n",
    "\n",
    "def find_closest_example(X_test_matrix,y_test_matrix):\n",
    "    train_size = phish_train_idx.shape[0] + X_train_legit.shape[0]\n",
    "    X_all_train = np.concatenate((X_train_phish_features,X_train_legit_features))\n",
    "    pairwise_distance = np.zeros([X_test_matrix.shape[0],train_size])\n",
    "    pairwise_distance_idx = np.zeros([X_test_matrix.shape[0],2])\n",
    "    \n",
    "    for i in range(0,X_test_matrix.shape[0]):\n",
    "        pair1 = X_test_matrix[i,:]\n",
    "        start_in_train = int(labels_start_end_train_legit[int(y_test_matrix[i]),0])\n",
    "        end_in_train = int(labels_start_end_train_legit[int(y_test_matrix[i]),1])\n",
    "        dist_list = []\n",
    "        for j in range(start_in_train,end_in_train+1):\n",
    "            pair2 = X_train_legit_features[j,:]\n",
    "            l2_diff = compute_distance_pair(pair1,pair2)\n",
    "            dist_list.append(l2_diff)\n",
    "            \n",
    "        min1 = min(dist_list)\n",
    "        min1_idx = start_in_train + argmin(dist_list)\n",
    "        \n",
    "        dist_list2 = []\n",
    "        start_in_phish_train = int(labels_start_end_train_phish[int(y_test_matrix[i]),0])\n",
    "        end_in_phish_train = int(labels_start_end_train_phish[int(y_test_matrix[i]),1])\n",
    "        \n",
    "        min2 = -1\n",
    "        if not labels_start_end_train_phish[int(y_test_matrix[i]),0] == -1:\n",
    "            for j in range(start_in_phish_train,end_in_phish_train+1):\n",
    "                pair2 = X_train_phish_features[j,:]\n",
    "                l2_diff = compute_distance_pair(pair1,pair2)\n",
    "                dist_list2.append(l2_diff)\n",
    "            \n",
    "            min2 = min(dist_list2)\n",
    "            min2_idx = argmin(dist_list2) + start_in_phish_train\n",
    "        \n",
    "        if min1 < min2 or min2 == -1:\n",
    "            pairwise_distance_idx[i,0] = min1_idx\n",
    "        else:\n",
    "            pairwise_distance_idx[i,0] = min2_idx\n",
    "            #min is from phishing train\n",
    "            pairwise_distance_idx[i,1] = 1\n",
    "            \n",
    "    return pairwise_distance_idx\n",
    "\n",
    "pairwise_distance_idx = find_closest_example(X_test_phish_features,y_test_phish_non_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adv examples generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adv_example(triple,epsilon):\n",
    "    \n",
    "    # Initialize adversarial example \n",
    "    anchor_adv = np.zeros_like(triple[0])\n",
    "    # Added noise\n",
    "    anchor_noise = np.zeros_like(triple[0])\n",
    "\n",
    "    y_true = tf.placeholder(\"float\", [None,1])\n",
    "    target = np.zeros([len(triple),1])\n",
    "    target.astype(float)\n",
    "    \n",
    "    # Get the loss and gradient of the loss wrt the inputs\n",
    "    loss_val = my_loss(y_true, full_model.output)\n",
    "    grads = K.gradients(loss_val, full_model.input[0])\n",
    "    \n",
    "    # Get the sign of the gradient\n",
    "    delta = K.sign(grads[0])\n",
    "    \n",
    "    dict_input = {y_true:target,full_model.input[0]:triple[0],full_model.input[1]:triple[1],full_model.input[2]:triple[2] }\n",
    "    delta1 = sess.run(delta, feed_dict=dict_input)\n",
    "    \n",
    "    # Get noise\n",
    "    anchor_noise = anchor_noise + delta1\n",
    "    \n",
    "    # Perturb the image\n",
    "    anchor_adv = triple[0] + epsilon*delta1\n",
    "    \n",
    "    return anchor_noise,anchor_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save adv. examples for the phishing test set \n",
    "# For each query image, pick the closest example as the positive image\n",
    "# Compute 5 trials (by randomly changing triplets)\n",
    "# Save the embeddings for each trial \n",
    "\n",
    "X_test_phish_non_ordered = all_imgs_test[phish_test_idx,:]\n",
    "y_test_phish_non_ordered = all_labels_test[phish_test_idx,:]\n",
    "\n",
    "X_test_phish_adv = np.zeros_like(X_test_phish_non_ordered)\n",
    "\n",
    "# initialize 3 empty arrays for the input image batch\n",
    "h = X_train_legit.shape[1]\n",
    "w = X_train_legit.shape[2]\n",
    "triple=[np.zeros((batch_size, h, w,3)) for i in range(3)]\n",
    "\n",
    "for l in range(0,trials):\n",
    "    number_batches = int(X_test_phish_non_ordered.shape[0]/batch_size)\n",
    "    count = 0\n",
    "    for i in range(0,number_batches):\n",
    "        for j in range(0,batch_size):\n",
    "            first_img = X_test_phish_non_ordered[i*batch_size+j,:]\n",
    "            triple[0][j,:,:,:] = first_img\n",
    "            first_img_label = int(y_test_phish_non_ordered[i*batch_size+j,:])\n",
    "\n",
    "\n",
    "            # get pos image by finding the closest image.\n",
    "            if pairwise_distance_idx[i*batch_size+j,1] == 1:\n",
    "                #from phish_train \n",
    "                pos_img = X_train_phish[int(pairwise_distance_idx[i*batch_size+j,0])]\n",
    "            else:\n",
    "                pos_img = X_train_legit[int(pairwise_distance_idx[i*batch_size+j,0])]\n",
    "            \n",
    "            triple[1][j,:,:,:] = pos_img\n",
    "\n",
    "            #get image for the thrid: negative from legit\n",
    "            neg_img,label_neg = pick_neg_img(first_img_label,155)\n",
    "            while check_if_same_category(first_img_label,label_neg) == 1:\n",
    "                neg_img,label_neg = pick_neg_img(first_img_label,155)\n",
    "            triple[2][j,:,:,:] = neg_img\n",
    "\n",
    "        anchor_noise,anchor_adv = get_adv_example(triple,epsilon)\n",
    "        for k in range(0,len(anchor_adv)):\n",
    "            X_test_phish_adv[count,:] = anchor_adv[k,:]\n",
    "            count = count + 1\n",
    "    X_test_phish_adv_features = inside_model.predict(X_test_phish_adv)\n",
    "    np.save(output_dir + 'X_test_phish_adv_closest_'+str(l),X_test_phish_adv_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
