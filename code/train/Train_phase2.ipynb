{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D,Conv2D,MaxPooling2D,Input,Lambda,GlobalMaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage.io import imsave\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters \n",
    "dataset_path = '../datasets/VisualPhish/VisualPhish/'\n",
    "reshape_size = [224,224,3]\n",
    "phishing_test_size = 0.4\n",
    "num_targets = 155 \n",
    "\n",
    "# Model parameters\n",
    "input_shape = [224,224,3]\n",
    "margin = 2.2\n",
    "new_conv_params = [5,5,512]\n",
    "\n",
    "# Training parameters\n",
    "start_lr = 0.00002\n",
    "output_dir = './'\n",
    "saved_model_name = 'model' #from first training \n",
    "new_saved_model_name = 'model2'\n",
    "save_interval = 2000\n",
    "batch_size = 32 \n",
    "n_iter = 50000\n",
    "lr_interval = 250\n",
    "# hard examples training \n",
    "num_sets = 100\n",
    "iter_per_set = 8\n",
    "n_iter = 30\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset:\n",
    "- Load training screenshots per website\n",
    "- Load Phishing screenshots per website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_imgs_per_website(data_path,targets,imgs_num,reshape_size,start_target_count):\n",
    "    all_imgs = np.zeros(shape=[imgs_num,224,224,3])\n",
    "    all_labels = np.zeros(shape=[imgs_num,1])\n",
    "    \n",
    "    all_file_names = []\n",
    "    targets_list = targets.splitlines()\n",
    "    count = 0\n",
    "    for i in range(0,len(targets_list)):\n",
    "        target_path = data_path + targets_list[i]\n",
    "        print(target_path)\n",
    "        file_names = sorted(os.listdir(target_path))\n",
    "        for j in range(0,len(file_names)):\n",
    "            try:\n",
    "                img = imread(target_path+'/'+file_names[j])\n",
    "                img = img[:,:,0:3]\n",
    "                all_imgs[count,:,:,:] = resize(img, (reshape_size[0], reshape_size[1]),anti_aliasing=True)\n",
    "                all_labels[count,:] = i + start_target_count\n",
    "                all_file_names.append(file_names[j])\n",
    "                count = count + 1\n",
    "            except:\n",
    "                #some images were saved with a wrong extensions \n",
    "                try:\n",
    "                    img = imread(target_path+'/'+file_names[j],format='jpeg')\n",
    "                    img = img[:,:,0:3]\n",
    "                    all_imgs[count,:,:,:] = resize(img, (reshape_size[0], reshape_size[1]),anti_aliasing=True)\n",
    "                    all_labels[count,:] = i + start_target_count\n",
    "                    all_file_names.append(file_names[j])\n",
    "                    count = count + 1\n",
    "                except:\n",
    "                    print('failed at:')\n",
    "                    print('***')\n",
    "                    print(file_names[j])\n",
    "                    break \n",
    "    return all_imgs,all_labels,all_file_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images legit (train)\n",
    "data_path = dataset_path + 'trusted_list/'\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "imgs_num = 9363\n",
    "all_imgs_train,all_labels_train,all_file_names_train = read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "# Read images phishing\n",
    "data_path = dataset_path + 'phishing/'\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "imgs_num = 1195\n",
    "all_imgs_test,all_labels_test,all_file_names_test = read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "X_train_legit = all_imgs_train\n",
    "y_train_legit = all_labels_train\n",
    "\n",
    "# Load the same train/split in phase 1\n",
    "phish_test_idx = np.load(output_dir+'test_idx.npy')\n",
    "phish_train_idx = np.load(output_dir+'train_idx.npy')\n",
    "\n",
    "X_test_phish = all_imgs_test[phish_test_idx,:]\n",
    "y_test_phish = all_labels_test[phish_test_idx,:]\n",
    "\n",
    "X_train_phish = all_imgs_test[phish_train_idx,:]\n",
    "y_train_phish = all_labels_test[phish_train_idx,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order and label targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_random_array(orig_arr,y_orig_arr,targets):\n",
    "    sorted_arr = np.zeros(orig_arr.shape)\n",
    "    y_sorted_arr = np.zeros(y_orig_arr.shape)\n",
    "    count = 0\n",
    "    for i in range(0,targets):\n",
    "        for j in range(0,orig_arr.shape[0]):\n",
    "            if y_orig_arr[j] == i:\n",
    "                sorted_arr[count,:,:,:] = orig_arr[j,:,:,:]\n",
    "                y_sorted_arr[count,:] = i\n",
    "                count = count + 1\n",
    "    return sorted_arr,y_sorted_arr \n",
    "\n",
    "# Store the start and end of each target in the phishing set (used later in triplet sampling)\n",
    "# Not all targets might be in the phishing set \n",
    "def start_end_each_target(num_target,labels):\n",
    "    prev_target = 0\n",
    "    start_end_each_target = np.zeros((num_target,2))\n",
    "    start_end_each_target[0,0] = 0\n",
    "    count_target = 0\n",
    "    for i in range(1,labels.shape[0]):\n",
    "        if not labels[i] == prev_target:\n",
    "            start_end_each_target[count_target,1] = i-1\n",
    "            count_target = count_target + 1\n",
    "            start_end_each_target[count_target,0] = i\n",
    "            prev_target = prev_target + 1\n",
    "    start_end_each_target[num_target-1,1] = labels.shape[0]-1\n",
    "    return start_end_each_target\n",
    "\n",
    "\n",
    "# Store the start and end of each target in the training set (used later in triplet sampling)\n",
    "def all_targets_start_end(num_target,labels):\n",
    "    prev_target = labels[0]\n",
    "    start_end_each_target = np.zeros((num_target,2))\n",
    "    start_end_each_target[0,0] = labels[0]\n",
    "    if not labels[0] == 0:\n",
    "        start_end_each_target[0,0] = -1\n",
    "        start_end_each_target[0,1] = -1\n",
    "    count_target = 0\n",
    "    for i in range(1,labels.shape[0]):\n",
    "        if not labels[i] == prev_target:\n",
    "            start_end_each_target[int(labels[i-1]),1] = int(i-1)\n",
    "            #count_target = count_target + 1\n",
    "            start_end_each_target[int(labels[i]),0] = int(i)\n",
    "            prev_target = labels[i]\n",
    "    start_end_each_target[int(labels[-1]),1] = int(labels.shape[0]-1)\n",
    "    \n",
    "    for i in range(1,num_target):\n",
    "        if start_end_each_target[i,0] == 0:\n",
    "            print(i)\n",
    "            start_end_each_target[i,0] = -1\n",
    "            start_end_each_target[i,1] = -1\n",
    "    return start_end_each_target\n",
    "\n",
    "labels_start_end_train_legit = all_targets_start_end(num_targets,y_train_legit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Hard subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a query set for each target\n",
    "def find_fixed_set_idx(num_target):\n",
    "    website_random_idx = np.zeros([num_target,])\n",
    "    for i in range(0,num_target):\n",
    "        class_idx_start_end = labels_start_end_train_legit[i,:]\n",
    "        website_random_idx[i] = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "    return website_random_idx\n",
    "\n",
    "# Compute L2 distance between embeddings\n",
    "def compute_distance_pair(layer1,layer2):\n",
    "    diff = layer1 - layer2\n",
    "    l2_diff = np.mean(diff**2)\n",
    "    return l2_diff\n",
    "    \n",
    "# Compute the embeddings of the query set, the phishing training set, the training whitelist \n",
    "def predict_all_imgs(model):\n",
    "    X_train_legit_last_layer = model.predict(X_train_legit,batch_size=10)\n",
    "    X_train_phish_last_layer = model.predict(X_train_phish,batch_size=10)\n",
    "    fixed_set_last_layer = model.predict(fixed_set,batch_size=10)\n",
    "    \n",
    "    return X_train_legit_last_layer,X_train_phish_last_layer,fixed_set_last_layer\n",
    "\n",
    "# Compute distance between the query set and all training examples \n",
    "def compute_all_distances(fixed_set,train_legit,train_phish):\n",
    "    train_size = train_legit.shape[0] + train_phish.shape[0]\n",
    "    X_all_train = np.concatenate((train_legit,train_phish))\n",
    "    pairwise_distance = np.zeros([fixed_set.shape[0],train_size])\n",
    "    for i in range(0,fixed_set.shape[0]):\n",
    "        pair1 = fixed_set[i,:]\n",
    "        for j in range(0,train_size):\n",
    "            pair2 = X_all_train[j,:]\n",
    "            l2_diff = compute_distance_pair(pair1,pair2)\n",
    "            pairwise_distance[i,j] = l2_diff\n",
    "    return pairwise_distance\n",
    "\n",
    "# Get index of false positives (different-website examples with small distance) of one query image\n",
    "def find_n_false_positives(distances,n,test_label):\n",
    "    count = 0\n",
    "    X_false_pos_idx = np.zeros([n,])\n",
    "    idx_min = np.argsort(distances)\n",
    "    for i in range(0,distances.shape[0]):\n",
    "        next_min_idx = idx_min[i]\n",
    "        n_label = y_train[next_min_idx]\n",
    "        #false positives (have close distance even if they are from differenet category)\n",
    "        if not (test_label == n_label):\n",
    "            X_false_pos_idx[count] = next_min_idx\n",
    "            count = count + 1\n",
    "            if count == n:\n",
    "                break \n",
    "    while count < n:\n",
    "        idx_min[count] = -1\n",
    "        count = count + 1\n",
    "    return X_false_pos_idx\n",
    "\n",
    "# Get index of false negatives (same-website examples with large distance) of one query image\n",
    "def find_n_false_negatives(distances,n,test_label):\n",
    "    count = 0     \n",
    "    X_false_neg_idx = np.zeros([n,])\n",
    "    idx_max = np.argsort(distances)[::-1]\n",
    "    for i in range(0,distances.shape[0]):\n",
    "        next_max_idx = idx_max[i]\n",
    "        n_label = y_train[next_max_idx]\n",
    "        #false negatives (have large distance although they are in the same category )\n",
    "        if test_label == n_label:\n",
    "            X_false_neg_idx[count] = next_max_idx\n",
    "            count = count + 1\n",
    "            if count == n:\n",
    "                break\n",
    "    while count < n:\n",
    "        idx_max[count] = -1\n",
    "        count = count + 1\n",
    "    return X_false_neg_idx\n",
    "\n",
    "# Get the idx of false positives and false negtaives for all query examples\n",
    "def find_index_for_all_set(distances,n):\n",
    "    all_idx = np.zeros([distances.shape[0],2,n])\n",
    "    for i in range(0,distances.shape[0]):\n",
    "        distance_i = distances[i,:]\n",
    "        all_idx[i,0,:] = find_n_false_positives(distance_i,n,i)\n",
    "        all_idx[i,1,:] = find_n_false_negatives(distance_i,n,i)\n",
    "    return all_idx\n",
    "\n",
    "# Form the new training set based on the hard examples indices of all query images\n",
    "def find_next_training_set(all_idx,n):\n",
    "    global X_train_new, y_train_new\n",
    "    all_idx = all_idx.astype(int)\n",
    "    count = 0\n",
    "    for i in range(all_idx.shape[0]):\n",
    "        for j in range(0,n):\n",
    "            if not all_idx[i,0,j] == -1:\n",
    "                X_train_new[count,:,:,:] = X_train[all_idx[i,0,j],:,:,:]\n",
    "                y_train_new[count,:] = y_train[all_idx[i,0,j]]\n",
    "                count = count +1 \n",
    "        for j in range(0,n):\n",
    "            if not all_idx[i,1,j] == -1:\n",
    "                X_train_new[count,:,:,:] = X_train[all_idx[i,1,j],:,:,:]\n",
    "                y_train_new[count,:] = y_train[all_idx[i,1,j]]\n",
    "                count = count +1 \n",
    "    X_train_new = X_train_new[0:count,:]\n",
    "    y_train_new = y_train_new[0:count,:]\n",
    "    return X_train_new,y_train_new\n",
    "\n",
    "# Main function for subset sampling\n",
    "# Steps:\n",
    "    # Predict all images\n",
    "    # Find pairwise distances between query and training set\n",
    "    # Find indices of hard positive and negative examples\n",
    "    # Find new training set\n",
    "    # Order training set by targets\n",
    "def find_main_train(model,fixed_set,targets):\n",
    "    X_train_legit_last_layer,X_train_phish_last_layer,fixed_set_last_layer = predict_all_imgs(model)\n",
    "    pairwise_distance = compute_all_distances(fixed_set_last_layer,X_train_legit_last_layer,X_train_phish_last_layer)\n",
    "    n = 1\n",
    "    all_idx = find_index_for_all_set(pairwise_distance,n)\n",
    "    X_train_new,y_train_new = find_next_training_set(all_idx,n)\n",
    "    X_train_new,y_train_new = order_random_array(X_train_new,y_train_new,targets)\n",
    "    labels_start_end_train = start_end_each_target(targets,y_train_new)\n",
    "    return X_train_new,y_train_new,labels_start_end_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't sample negative image from the same category as the positive image (e.g. google and google drive)\n",
    "# Create clusters of same-company websites (e.g. all microsoft websites)\n",
    "\n",
    "\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "all_targets = targets_file.read()\n",
    "all_targets = all_targets.splitlines()\n",
    "\n",
    "def get_idx_of_target(target_name,all_targets):\n",
    "    for i in range(0,len(all_targets)):\n",
    "        if all_targets[i] == target_name:\n",
    "            found_idx = i\n",
    "            return found_idx\n",
    "        \n",
    "#targets names of parent and sub websites\n",
    "target_lists = [['microsoft','ms_outlook','ms_office','ms_bing','ms_onedrive','ms_skype'],['apple','itunes','icloud'],['google','google_drive'],['alibaba','aliexpress']]\n",
    "\n",
    "def get_associated_targets_idx(target_lists,all_targets):\n",
    "    sub_target_lists_idx = []\n",
    "    parents_ids = []\n",
    "    for i in range(0,len(target_lists)):\n",
    "        target_list = target_lists[i]\n",
    "        parent_target = target_list[0]\n",
    "        one_target_list = []\n",
    "        parent_idx = get_idx_of_target(parent_target,all_targets)\n",
    "        parents_ids.append(parent_idx)\n",
    "        for child_target in target_list[1:]:\n",
    "            child_idx = get_idx_of_target(child_target,all_targets)\n",
    "            one_target_list.append(child_idx)\n",
    "        sub_target_lists_idx.append(one_target_list)\n",
    "    return parents_ids,sub_target_lists_idx \n",
    "\n",
    "parents_ids,sub_target_lists_idx  = get_associated_targets_idx(target_lists,all_targets)\n",
    "\n",
    "def check_if_same_category(img_label1,img_label2):\n",
    "    if_same = 0\n",
    "    if img_label1 in parents_ids:\n",
    "        if img_label2 in sub_target_lists_idx[parents_ids.index(img_label1)]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[0]:\n",
    "        if img_label2 in sub_target_lists_idx[0] or img_label2 == parents_ids[0]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[1]:\n",
    "        if img_label2 in sub_target_lists_idx[1] or img_label2 == parents_ids[1]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[2]:\n",
    "        if img_label2 in sub_target_lists_idx[2] or img_label2 == parents_ids[2]:\n",
    "            if_same = 1\n",
    "    return if_same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_first_img_idx(labels_start_end,num_targets):\n",
    "    random_target = -1\n",
    "    while (random_target == -1):\n",
    "        random_target = np.random.randint(low = 0,high = num_targets)\n",
    "        if labels_start_end[random_target,0] == -1:\n",
    "            random_target = -1\n",
    "    return random_target\n",
    "\n",
    "def pick_pos_img_idx(img_label):\n",
    "    class_idx_start_end = labels_start_end_train[img_label,:]\n",
    "    same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "    img = X_train_new[same_idx,:]\n",
    "    return img\n",
    "\n",
    "def pick_neg_img(anchor_idx,num_targets):\n",
    "    if anchor_idx == 0:\n",
    "        targets = np.arange(1,num_targets)\n",
    "    elif anchor_idx == num_targets -1:\n",
    "        targets = np.arange(0,num_targets-1)\n",
    "    else:\n",
    "        targets = np.concatenate([np.arange(0,anchor_idx),np.arange(anchor_idx+1,num_targets)])\n",
    "    diff_target_idx = np.random.randint(low = 0,high = num_targets-1)\n",
    "    diff_target = targets[diff_target_idx]\n",
    "    \n",
    "    class_idx_start_end = labels_start_end_train[diff_target,:]\n",
    "    idx_from_diff_target = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "    img = X_train_new[idx_from_diff_target,:]\n",
    "    \n",
    "    return img,diff_target\n",
    "\n",
    "#Sample batch \n",
    "def get_batch(batch_size,train_fixed_set,num_targets):\n",
    "   \n",
    "    # initialize 3 empty arrays for the input image batch\n",
    "    h = X_train_legit.shape[1]\n",
    "    w = X_train_legit.shape[2]\n",
    "    triple=[np.zeros((batch_size, h, w,3)) for i in range(3)]\n",
    "\n",
    "    for i in range(0,batch_size):\n",
    "        img_idx_pair1 = pick_first_img_idx(labels_start_end_train,num_targets)\n",
    "        triple[0][i,:,:,:] = train_fixed_set[img_idx_pair1,:]\n",
    "        img_label = img_idx_pair1\n",
    "        \n",
    "        #get image for the second: positive\n",
    "        triple[1][i,:,:,:] = pick_pos_img_idx(img_label)\n",
    "            \n",
    "        #get image for the thrid: negative from legit\n",
    "        img_neg,label_neg = pick_neg_img(img_label,num_targets)\n",
    "        while check_if_same_category(img_label,label_neg) == 1:\n",
    "            img_neg,label_neg = pick_neg_img(img_label,num_targets)\n",
    "\n",
    "        triple[2][i,:,:,:] = img_neg\n",
    "          \n",
    "    return triple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss(y_true,y_pred):\n",
    "    loss_value = K.maximum(y_true, margin + y_pred)\n",
    "    loss_value = K.mean(loss_value,axis=0)\n",
    "    return loss_value\n",
    "\n",
    "full_model = load_model(output_dir+saved_model_name+'.h5', custom_objects={'loss': loss})\n",
    "\n",
    "def custom_loss(margin):\n",
    "    def loss(y_true,y_pred):\n",
    "        loss_value = K.maximum(y_true, margin + y_pred)\n",
    "        loss_value = K.mean(loss_value,axis=0)\n",
    "        return loss_value\n",
    "    return loss\n",
    "\n",
    "\n",
    "from keras import optimizers\n",
    "optimizer = optimizers.Adam(lr = start_lr)\n",
    "full_model.compile(loss=custom_loss(margin),optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keras_model(model):\n",
    "    model.save(output_dir+new_saved_model_name+'.h5')\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1 #number of wrong points \n",
    "\n",
    "#all training images\n",
    "X_train = np.concatenate([X_train_legit,X_train_phish])\n",
    "y_train = np.concatenate([y_train_legit,y_train_phish])\n",
    "\n",
    "#subset training\n",
    "X_train_new = np.zeros([num_targets*2*n,X_train_legit.shape[1],X_train_legit.shape[2],X_train_legit.shape[3]])\n",
    "y_train_new = np.zeros([num_targets*2*n,1])\n",
    "\n",
    "targets_train = np.zeros([batch_size,1])\n",
    "tot_count = 0 \n",
    "\n",
    "print(\"Starting training process!\")\n",
    "print(\"\\n ------------- \\n\")\n",
    "for k in range(0,num_sets):\n",
    "    print(\"Starting a new set!\")\n",
    "    print(\"\\n ------------- \\n\")\n",
    "    X_train_legit = all_imgs_train\n",
    "    y_train_legit = all_labels_train\n",
    "    \n",
    "    fixed_set_idx = find_fixed_set_idx(num_targets)\n",
    "    fixed_set = X_train_legit[fixed_set_idx.astype(int),:,:,:]\n",
    "    \n",
    "    for j in range(0,iter_per_set):\n",
    "        model = full_model.layers[3]\n",
    "        X_train_new,y_train_new,labels_start_end_train = find_main_train(model,fixed_set,num_targets)\n",
    "        for i in range(1, n_iter):\n",
    "            tot_count = tot_count + 1\n",
    "            inputs=get_batch(batch_size,fixed_set,num_targets)\n",
    "            loss_iteration=full_model.train_on_batch(inputs,targets_train)\n",
    "            \n",
    "            print(\"\\n ------------- \\n\")\n",
    "            print('Iteration: '+ str(i) +'. '+ \"Loss: {0}\".format(loss_iteration))\n",
    "\n",
    "            if tot_count % save_interval == 0:\n",
    "                save_keras_model(full_model)\n",
    "\n",
    "            if tot_count % lr_interval ==0:\n",
    "                start_lr = 0.99*start_lr\n",
    "                K.set_value(full_model.optimizer.lr, start_lr)\n",
    "\n",
    "save_keras_model(full_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_model = full_model.layers[3]\n",
    "\n",
    "whitelist_emb = shared_model.predict(X_train_legit,batch_size=64)\n",
    "np.save(output_dir+'whitelist_emb2',whitelist_emb)\n",
    "np.save(output_dir+'whitelist_labels2',y_train_legit )\n",
    "\n",
    "phishing_emb = shared_model.predict(all_imgs_test,batch_size=64)\n",
    "np.save(output_dir+'phishing_emb2',phishing_emb)\n",
    "np.save(output_dir+'phishing_labels2',all_labels_test )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
