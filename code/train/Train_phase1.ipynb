{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten,Subtract,Reshape\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D,Conv2D,MaxPooling2D,Input,Lambda,GlobalMaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from skimage.io import imsave\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import rescale, resize\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "\n",
    "print(\"Keras Version:      \"+keras.__version__)\n",
    "print(\"Tensorflow Version: \"+tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters \n",
    "dataset_path = '../../datasets/VisualPhish/'\n",
    "reshape_size = [224,224,3]\n",
    "phishing_test_size = 0.4\n",
    "num_targets = 155 \n",
    "\n",
    "# Model parameters\n",
    "input_shape = [224,224,3]\n",
    "margin = 2.2\n",
    "new_conv_params = [3,3,512]\n",
    "\n",
    "# Training parameters\n",
    "start_lr = 0.00002\n",
    "output_dir = './'\n",
    "saved_model_name = 'model'\n",
    "save_interval = 2000\n",
    "batch_size = 32 \n",
    "n_iter = 50000\n",
    "lr_interval = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset:\n",
    "    - Load training screenshots per website\n",
    "    - Load Phishing screenshots per website "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_imgs_per_website(data_path,targets,imgs_num,reshape_size,start_target_count):\n",
    "    all_imgs = np.zeros(shape=[imgs_num,224,224,3])\n",
    "    all_labels = np.zeros(shape=[imgs_num,1])\n",
    "    \n",
    "    all_file_names = []\n",
    "    targets_list = targets.splitlines()\n",
    "    count = 0\n",
    "    for i in range(0,len(targets_list)):\n",
    "        target_path = data_path + targets_list[i]\n",
    "        print(target_path)\n",
    "        file_names = sorted(os.listdir(target_path))\n",
    "        for j in range(0,len(file_names)):\n",
    "            try:\n",
    "                img = imread(target_path+'/'+file_names[j])\n",
    "                img = img[:,:,0:3]\n",
    "                all_imgs[count,:,:,:] = resize(img, (reshape_size[0], reshape_size[1]),anti_aliasing=True)\n",
    "                all_labels[count,:] = i + start_target_count\n",
    "                all_file_names.append(file_names[j])\n",
    "                count = count + 1\n",
    "            except:\n",
    "                #some images were saved with a wrong extensions \n",
    "                try:\n",
    "                    img = imread(target_path+'/'+file_names[j],format='jpeg')\n",
    "                    img = img[:,:,0:3]\n",
    "                    all_imgs[count,:,:,:] = resize(img, (reshape_size[0], reshape_size[1]),anti_aliasing=True)\n",
    "                    all_labels[count,:] = i + start_target_count\n",
    "                    all_file_names.append(file_names[j])\n",
    "                    count = count + 1\n",
    "                except:\n",
    "                    print('failed at:')\n",
    "                    print('***')\n",
    "                    print(file_names[j])\n",
    "                    break \n",
    "    return all_imgs,all_labels,all_file_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images legit (train)\n",
    "data_path = dataset_path + 'trusted_list/'\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "imgs_num = 9363\n",
    "all_imgs_train,all_labels_train,all_file_names_train = read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "# Read images phishing\n",
    "data_path = dataset_path + 'phishing/'\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "imgs_num = 1195\n",
    "all_imgs_test,all_labels_test,all_file_names_test = read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "X_train_legit = all_imgs_train\n",
    "y_train_legit = all_labels_train\n",
    "\n",
    "# Split phishing to training and test \n",
    "idx = np.arange(all_imgs_test.shape[0])\n",
    "X_test_phish, X_train_phish, y_test_phish, y_train_phish,idx_test,idx_train = train_test_split(all_imgs_test, all_labels_test,idx, test_size=phishing_test_size)\n",
    "np.save(output_dir+'test_idx',idx_test)\n",
    "np.save(output_dir+'train_idx',idx_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_triplet_network(input_shape, new_conv_params):\n",
    "    \n",
    "    # Input_shape: shape of input images\n",
    "    # new_conv_params: dimension of the new convolution layer [spatial1,spatial2,channels] \n",
    "    \n",
    "    # Define the tensors for the three input images\n",
    "    anchor_input = Input(input_shape)\n",
    "    positive_input = Input(input_shape)\n",
    "    negative_input = Input(input_shape)\n",
    "    \n",
    "    # Use VGG as a base model \n",
    "    base_model = VGG16(weights='imagenet',  input_shape=input_shape, include_top=False)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Conv2D(new_conv_params[2],(new_conv_params[0],new_conv_params[1]),activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(2e-4)) (x)\n",
    "    x = GlobalAveragePooling2D() (x)\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_a = model(anchor_input)\n",
    "    encoded_p = model(positive_input)\n",
    "    encoded_n = model(negative_input)\n",
    "    \n",
    "    mean_layer = Lambda(lambda x: K.mean(x,axis=1))\n",
    "    \n",
    "    square_diff_layer = Lambda(lambda tensors:K.square(tensors[0] - tensors[1]))\n",
    "    square_diff_pos = square_diff_layer([encoded_a,encoded_p])\n",
    "    square_diff_neg = square_diff_layer([encoded_a,encoded_n])\n",
    "    \n",
    "    square_diff_pos_l2 = mean_layer(square_diff_pos)\n",
    "    square_diff_neg_l2 = mean_layer(square_diff_neg)\n",
    "    \n",
    "    # Add a diff layer\n",
    "    diff = Subtract()([square_diff_pos_l2, square_diff_neg_l2])\n",
    "    diff = Reshape((1,)) (diff)\n",
    "\n",
    "    # Connect the inputs with the outputs\n",
    "    triplet_net = Model(inputs=[anchor_input,positive_input,negative_input],outputs=diff)\n",
    "    \n",
    "    # return the model\n",
    "    return triplet_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the triplet loss\n",
    "# Create and compile model \n",
    "\n",
    "def custom_loss(margin):\n",
    "    def loss(y_true,y_pred):\n",
    "        loss_value = K.maximum(y_true, margin + y_pred)\n",
    "        loss_value = K.mean(loss_value,axis=0)\n",
    "        return loss_value\n",
    "    return loss\n",
    "def loss(y_true,y_pred):\n",
    "    loss_value = K.maximum(y_true, margin + y_pred)\n",
    "    loss_value = K.mean(loss_value,axis=0)\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "model = define_triplet_network(input_shape, new_conv_params)\n",
    "model.summary()\n",
    "\n",
    "from keras import optimizers\n",
    "optimizer = optimizers.Adam(lr = start_lr)\n",
    "model.compile(loss=custom_loss(margin),optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order random phishing arrays per website (from 0 to 155 target)\n",
    "\n",
    "def order_random_array(orig_arr,y_orig_arr,targets):\n",
    "    sorted_arr = np.zeros(orig_arr.shape)\n",
    "    y_sorted_arr = np.zeros(y_orig_arr.shape)\n",
    "    count = 0\n",
    "    for i in range(0,targets):\n",
    "        for j in range(0,orig_arr.shape[0]):\n",
    "            if y_orig_arr[j] == i:\n",
    "                sorted_arr[count,:,:,:] = orig_arr[j,:,:,:]\n",
    "                y_sorted_arr[count,:] = i\n",
    "                count = count + 1\n",
    "    return sorted_arr,y_sorted_arr \n",
    "\n",
    "X_test_phish,y_test_phish = order_random_array(X_test_phish,y_test_phish,num_targets)\n",
    "X_train_phish,y_train_phish = order_random_array(X_train_phish,y_train_phish,num_targets)\n",
    "\n",
    "\n",
    "# Store the start and end of each target in the phishing set (used later in triplet sampling)\n",
    "# Not all targets might be in the phishing set \n",
    "def targets_start_end(num_target,labels):\n",
    "    prev_target = labels[0]\n",
    "    start_end_each_target = np.zeros((num_target,2))\n",
    "    start_end_each_target[0,0] = labels[0]\n",
    "    if not labels[0] == 0:\n",
    "        start_end_each_target[0,0] = -1\n",
    "        start_end_each_target[0,1] = -1\n",
    "    count_target = 0\n",
    "    for i in range(1,labels.shape[0]):\n",
    "        if not labels[i] == prev_target:\n",
    "            start_end_each_target[int(labels[i-1]),1] = int(i-1)\n",
    "            start_end_each_target[int(labels[i]),0] = int(i)\n",
    "            prev_target = labels[i]\n",
    "    start_end_each_target[int(labels[-1]),1] = int(labels.shape[0]-1)\n",
    "    \n",
    "    for i in range(1,num_target):\n",
    "        if start_end_each_target[i,0] == 0:\n",
    "            start_end_each_target[i,0] = -1\n",
    "            start_end_each_target[i,1] = -1\n",
    "    return start_end_each_target\n",
    "\n",
    "labels_start_end_train_phish = targets_start_end(num_targets,y_train_phish)\n",
    "labels_start_end_test_phish = targets_start_end(num_targets,y_test_phish)\n",
    "\n",
    "\n",
    "# Store the start and end of each target in the training set (used later in triplet sampling)\n",
    "def all_targets_start_end(num_target,labels):\n",
    "    prev_target = 0\n",
    "    start_end_each_target = np.zeros((num_target,2))\n",
    "    start_end_each_target[0,0] = 0\n",
    "    count_target = 0\n",
    "    for i in range(1,labels.shape[0]):\n",
    "        if not labels[i] == prev_target:\n",
    "            start_end_each_target[count_target,1] = i-1\n",
    "            count_target = count_target + 1\n",
    "            start_end_each_target[count_target,0] = i\n",
    "            prev_target = prev_target + 1\n",
    "    start_end_each_target[num_target-1,1] = labels.shape[0]-1\n",
    "    return start_end_each_target\n",
    "\n",
    "labels_start_end_train_legit = all_targets_start_end(num_targets,y_train_legit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample anchor, positive and negative images \n",
    "def pick_first_img_idx(labels_start_end,num_targets):\n",
    "    random_target = -1\n",
    "    while (random_target == -1):\n",
    "        random_target = np.random.randint(low = 0,high = num_targets)\n",
    "        if labels_start_end[random_target,0] == -1:\n",
    "            random_target = -1\n",
    "    return random_target\n",
    "\n",
    "def pick_pos_img_idx(prob_phish,img_label):\n",
    "    if np.random.uniform() > prob_phish:\n",
    "        class_idx_start_end = labels_start_end_train_legit[img_label,:]\n",
    "        same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "        img = X_train_legit[same_idx,:]\n",
    "    else:\n",
    "        if not labels_start_end_train_phish[img_label,0] == -1:\n",
    "            class_idx_start_end = labels_start_end_train_phish[img_label,:]\n",
    "            same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "            img = X_train_phish[same_idx,:]\n",
    "        else:\n",
    "            class_idx_start_end = labels_start_end_train_legit[img_label,:]\n",
    "            same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "            img = X_train_legit[same_idx,:]\n",
    "    return img\n",
    "\n",
    "def pick_neg_img(anchor_idx,num_targets):\n",
    "    if anchor_idx == 0:\n",
    "        targets = np.arange(1,num_targets)\n",
    "    elif anchor_idx == num_targets -1:\n",
    "        targets = np.arange(0,num_targets-1)\n",
    "    else:\n",
    "        targets = np.concatenate([np.arange(0,anchor_idx),np.arange(anchor_idx+1,num_targets)])\n",
    "    diff_target_idx = np.random.randint(low = 0,high = num_targets-1)\n",
    "    diff_target = targets[diff_target_idx]\n",
    "    \n",
    "    class_idx_start_end = labels_start_end_train_legit[diff_target,:]\n",
    "    idx_from_diff_target = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "    img = X_train_legit[idx_from_diff_target,:]\n",
    "    \n",
    "    return img,diff_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't sample negative image from the same category as the positive image (e.g. google and google drive)\n",
    "# Create clusters of same-company websites (e.g. all microsoft websites)\n",
    "\n",
    "\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "all_targets = targets_file.read()\n",
    "all_targets = all_targets.splitlines()\n",
    "\n",
    "def get_idx_of_target(target_name,all_targets):\n",
    "    for i in range(0,len(all_targets)):\n",
    "        if all_targets[i] == target_name:\n",
    "            found_idx = i\n",
    "            return found_idx\n",
    "        \n",
    "#targets names of parent and sub websites\n",
    "target_lists = [['microsoft','ms_outlook','ms_office','ms_bing','ms_onedrive','ms_skype'],['apple','itunes','icloud'],['google','google_drive'],['alibaba','aliexpress']]\n",
    "\n",
    "def get_associated_targets_idx(target_lists,all_targets):\n",
    "    sub_target_lists_idx = []\n",
    "    parents_ids = []\n",
    "    for i in range(0,len(target_lists)):\n",
    "        target_list = target_lists[i]\n",
    "        parent_target = target_list[0]\n",
    "        one_target_list = []\n",
    "        parent_idx = get_idx_of_target(parent_target,all_targets)\n",
    "        parents_ids.append(parent_idx)\n",
    "        for child_target in target_list[1:]:\n",
    "            child_idx = get_idx_of_target(child_target,all_targets)\n",
    "            one_target_list.append(child_idx)\n",
    "        sub_target_lists_idx.append(one_target_list)\n",
    "    return parents_ids,sub_target_lists_idx \n",
    "\n",
    "parents_ids,sub_target_lists_idx  = get_associated_targets_idx(target_lists,all_targets)\n",
    "\n",
    "def check_if_same_category(img_label1,img_label2):\n",
    "    if_same = 0\n",
    "    if img_label1 in parents_ids:\n",
    "        if img_label2 in sub_target_lists_idx[parents_ids.index(img_label1)]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[0]:\n",
    "        if img_label2 in sub_target_lists_idx[0] or img_label2 == parents_ids[0]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[1]:\n",
    "        if img_label2 in sub_target_lists_idx[1] or img_label2 == parents_ids[1]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[2]:\n",
    "        if img_label2 in sub_target_lists_idx[2] or img_label2 == parents_ids[2]:\n",
    "            if_same = 1\n",
    "    return if_same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample triplets\n",
    "def get_batch(batch_size,num_targets):\n",
    "   \n",
    "    # initialize 3 empty arrays for the input image batch\n",
    "    h = X_train_legit.shape[1]\n",
    "    w = X_train_legit.shape[2]\n",
    "    triple=[np.zeros((batch_size, h, w,3)) for i in range(3)]\n",
    "\n",
    "    for i in range(0,batch_size):\n",
    "        img_idx_pair1 = pick_first_img_idx(labels_start_end_train_legit,num_targets)\n",
    "        triple[0][i,:,:,:] = X_train_legit[img_idx_pair1,:]\n",
    "        img_label = int(y_train_legit[img_idx_pair1])\n",
    "        \n",
    "        # get image for the second: positive\n",
    "        triple[1][i,:,:,:] = pick_pos_img_idx(0.15,img_label)\n",
    "            \n",
    "        # get image for the thrid: negative from legit\n",
    "        # don't sample from the same cluster\n",
    "        img_neg,label_neg = pick_neg_img(img_label,num_targets)\n",
    "        while check_if_same_category(img_label,label_neg) == 1:\n",
    "            img_neg,label_neg = pick_neg_img(img_label,num_targets)\n",
    "\n",
    "        triple[2][i,:,:,:] = img_neg\n",
    "          \n",
    "    return triple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keras_model(model):\n",
    "    model.save(output_dir+saved_model_name+'.h5')\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "targets_train = np.zeros([batch_size,1])\n",
    "for i in range(1, n_iter):\n",
    "    inputs=get_batch(batch_size,num_targets)\n",
    "    loss_value=model.train_on_batch(inputs,targets_train)\n",
    "    \n",
    "\n",
    "    print(\"\\n ------------- \\n\")\n",
    "    print('Iteration: '+ str(i) +'. '+ \"Loss: {0}\".format(loss_value))\n",
    "    \n",
    "    if i % save_interval == 0:\n",
    "        save_keras_model(model)\n",
    "        \n",
    "    if i % lr_interval ==0:\n",
    "        start_lr = 0.99*start_lr\n",
    "        K.set_value(model.optimizer.lr, start_lr)\n",
    "\n",
    "save_keras_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_model = model.layers[3]\n",
    "\n",
    "whitelist_emb = shared_model.predict(X_train_legit,batch_size=64)\n",
    "np.save(output_dir+'whitelist_emb',whitelist_emb)\n",
    "np.save(output_dir+'whitelist_labels',y_train_legit )\n",
    "\n",
    "phishing_emb = shared_model.predict(all_imgs_test,batch_size=64)\n",
    "np.save(output_dir+'phishing_emb',phishing_emb)\n",
    "np.save(output_dir+'phishing_labels',all_labels_test )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
