{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten,Subtract,Reshape\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D,Conv2D,MaxPooling2D,Input,Lambda,GlobalMaxPooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from skimage.io import imsave\n",
    "\n",
    "from matplotlib.pyplot import imread\n",
    "from skimage.transform import rescale, resize\n",
    "import os\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "print(\"Keras Version:      \"+keras.__version__)\n",
    "print(\"Tensorflow Version: \"+tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 2.2\n",
    "start_lr = 0.00002\n",
    "\n",
    "dataset_path = '../../datasets/VisualPhish/'\n",
    "reshape_size = [224,224,3]\n",
    "num_targets = 155 \n",
    "batch_size = 32 \n",
    "n_iter = 6000\n",
    "\n",
    "input_shape = [224,224,3]\n",
    "saved_model_name = 'model.h5' #from first training \n",
    "\n",
    "new_saved_model_name = 'model_adv'\n",
    "output_dir = './'\n",
    "save_interval = 1000\n",
    "lr_interval = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_imgs_per_website(data_path,targets,imgs_num,reshape_size,start_target_count):\n",
    "    all_imgs = np.zeros(shape=[imgs_num,224,224,3])\n",
    "    all_labels = np.zeros(shape=[imgs_num,1])\n",
    "    \n",
    "    all_file_names = []\n",
    "    targets_list = targets.splitlines()\n",
    "    count = 0\n",
    "    for i in range(0,len(targets_list)):\n",
    "        target_path = data_path + targets_list[i]\n",
    "        print(target_path)\n",
    "        file_names = sorted(os.listdir(target_path))\n",
    "        for j in range(0,len(file_names)):\n",
    "            try:\n",
    "                img = imread(target_path+'/'+file_names[j])\n",
    "                img = img[:,:,0:3]\n",
    "                all_imgs[count,:,:,:] = resize(img, (reshape_size[0], reshape_size[1]),anti_aliasing=True)\n",
    "                all_labels[count,:] = i + start_target_count\n",
    "                all_file_names.append(file_names[j])\n",
    "                count = count + 1\n",
    "            except:\n",
    "                #some images were saved with a wrong extensions \n",
    "                try:\n",
    "                    img = imread(target_path+'/'+file_names[j],format='jpeg')\n",
    "                    img = img[:,:,0:3]\n",
    "                    all_imgs[count,:,:,:] = resize(img, (reshape_size[0], reshape_size[1]),anti_aliasing=True)\n",
    "                    all_labels[count,:] = i + start_target_count\n",
    "                    all_file_names.append(file_names[j])\n",
    "                    count = count + 1\n",
    "                except:\n",
    "                    print('failed at:')\n",
    "                    print('***')\n",
    "                    print(file_names[j])\n",
    "                    break \n",
    "    return all_imgs,all_labels,all_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images legit (train)\n",
    "data_path = dataset_path + 'trusted_list/'\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "imgs_num = 9363\n",
    "all_imgs_train,all_labels_train,all_file_names_train = read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "# Read images phishing\n",
    "data_path = dataset_path + 'phishing/'\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "targets = targets_file.read()\n",
    "imgs_num = 1195\n",
    "all_imgs_test,all_labels_test,all_file_names_test = read_imgs_per_website(data_path,targets,imgs_num,reshape_size,0)\n",
    "\n",
    "X_train_legit = all_imgs_train\n",
    "y_train_legit = all_labels_train\n",
    "\n",
    "# Load indices of training and test split\n",
    "idx_train = np.load(output_dir+'train_idx.npy')\n",
    "idx_test = np.load(output_dir+'test_idx.npy')\n",
    "X_test_phish = all_imgs_test[idx_test,:]\n",
    "y_test_phish = all_labels_test[idx_test,:]\n",
    "\n",
    "X_train_phish = all_imgs_test[idx_train,:]\n",
    "y_train_phish = all_labels_test[idx_train,:]\n",
    "\n",
    "def order_random_array(orig_arr,y_orig_arr,targets):\n",
    "    sorted_arr = np.zeros(orig_arr.shape)\n",
    "    y_sorted_arr = np.zeros(y_orig_arr.shape)\n",
    "    count = 0\n",
    "    for i in range(0,targets):\n",
    "        for j in range(0,orig_arr.shape[0]):\n",
    "            if y_orig_arr[j] == i:\n",
    "                sorted_arr[count,:,:,:] = orig_arr[j,:,:,:]\n",
    "                y_sorted_arr[count,:] = i\n",
    "                count = count + 1\n",
    "    return sorted_arr,y_sorted_arr \n",
    "\n",
    "X_test_phish,y_test_phish = order_random_array(X_test_phish,y_test_phish,155)\n",
    "X_train_phish,y_train_phish = order_random_array(X_train_phish,y_train_phish,155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get start and end of each label\n",
    "def start_end_each_target_not_complete(num_target,labels):\n",
    "    prev_target = labels[0]\n",
    "    start_end_each_target = np.zeros((num_target,2))\n",
    "    start_end_each_target[0,0] = labels[0]\n",
    "    if not labels[0] == 0:\n",
    "        start_end_each_target[0,0] = -1\n",
    "        start_end_each_target[0,1] = -1\n",
    "    count_target = 0\n",
    "    for i in range(1,labels.shape[0]):\n",
    "        if not labels[i] == prev_target:\n",
    "            start_end_each_target[int(labels[i-1]),1] = int(i-1)\n",
    "            #count_target = count_target + 1\n",
    "            start_end_each_target[int(labels[i]),0] = int(i)\n",
    "            prev_target = labels[i]\n",
    "    start_end_each_target[int(labels[-1]),1] = int(labels.shape[0]-1)\n",
    "    \n",
    "    for i in range(1,num_target):\n",
    "        if start_end_each_target[i,0] == 0:\n",
    "            start_end_each_target[i,0] = -1\n",
    "            start_end_each_target[i,1] = -1\n",
    "    return start_end_each_target\n",
    "\n",
    "labels_start_end_train_phish = start_end_each_target_not_complete(num_targets,y_train_phish)\n",
    "labels_start_end_test_phish = start_end_each_target_not_complete(num_targets,y_test_phish)\n",
    "\n",
    "\n",
    "def start_end_each_target(num_target,labels):\n",
    "    prev_target = 0\n",
    "    start_end_each_target = np.zeros((num_target,2))\n",
    "    start_end_each_target[0,0] = 0\n",
    "    count_target = 0\n",
    "    for i in range(1,labels.shape[0]):\n",
    "        if not labels[i] == prev_target:\n",
    "            start_end_each_target[count_target,1] = i-1\n",
    "            count_target = count_target + 1\n",
    "            start_end_each_target[count_target,0] = i\n",
    "            prev_target = prev_target + 1\n",
    "    start_end_each_target[num_target-1,1] = labels.shape[0]-1\n",
    "    return start_end_each_target\n",
    "\n",
    "labels_start_end_train_legit = start_end_each_target(num_targets,y_train_legit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(margin):\n",
    "    def loss(y_true,y_pred):\n",
    "        loss_value = K.maximum(y_true, margin + y_pred)\n",
    "        loss_value = K.mean(loss_value,axis=0)\n",
    "        return loss_value\n",
    "    return loss\n",
    "my_loss = custom_loss(30)\n",
    "\n",
    "def loss(y_true,y_pred):\n",
    "    loss_value = K.maximum(y_true, margin + y_pred)\n",
    "    loss_value = K.mean(loss_value,axis=0)\n",
    "    return loss_value\n",
    "\n",
    "model = load_model(output_dir+saved_model_name, custom_objects={'loss': loss})\n",
    "optimizer = optimizers.Adam(lr = start_lr)\n",
    "model.compile(loss=custom_loss(margin),optimizer=optimizer)\n",
    "model.summary()\n",
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_first_img_idx(labels_start_end,num_targets):\n",
    "    random_target = -1\n",
    "    while (random_target == -1):\n",
    "        random_target = np.random.randint(low = 0,high = num_targets)\n",
    "        if labels_start_end[random_target,0] == -1:\n",
    "            random_target = -1\n",
    "    class_idx_start_end = labels_start_end[random_target,:]\n",
    "    img_from_target_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "    return img_from_target_idx\n",
    "\n",
    "def pick_pos_img_idx(prob_phish,img_label):\n",
    "    if np.random.uniform() > prob_phish:\n",
    "        #take image from legit\n",
    "        class_idx_start_end = labels_start_end_train_legit[img_label,:]\n",
    "        same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "        img = X_train_legit[same_idx,:]\n",
    "    else:\n",
    "        #take from phish\n",
    "        if not labels_start_end_train_phish[img_label,0] == -1:\n",
    "            class_idx_start_end = labels_start_end_train_phish[img_label,:]\n",
    "            same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "            img = X_train_phish[same_idx,:]\n",
    "        else:\n",
    "            class_idx_start_end = labels_start_end_train_legit[img_label,:]\n",
    "            same_idx = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "            img = X_train_legit[same_idx,:]\n",
    "    return img\n",
    "\n",
    "def pick_neg_img(anchor_idx,num_targets):\n",
    "    if anchor_idx == 0:\n",
    "        targets = np.arange(1,num_targets)\n",
    "    elif anchor_idx == num_targets -1:\n",
    "        targets = np.arange(0,num_targets-1)\n",
    "    else:\n",
    "        targets = np.concatenate([np.arange(0,anchor_idx),np.arange(anchor_idx+1,num_targets)])\n",
    "    diff_target_idx = np.random.randint(low = 0,high = num_targets-1)\n",
    "    diff_target = targets[diff_target_idx]\n",
    "    \n",
    "    class_idx_start_end = labels_start_end_train_legit[diff_target,:]\n",
    "    idx_from_diff_target = np.random.randint(low = class_idx_start_end[0],high = class_idx_start_end[1]+1)\n",
    "    img = X_train_legit[idx_from_diff_target,:]\n",
    "    \n",
    "    return img,diff_target\n",
    "\n",
    "targets_file = open(data_path+'targets.txt', \"r\")\n",
    "all_targets = targets_file.read()\n",
    "all_targets = all_targets.splitlines()\n",
    "\n",
    "def get_idx_of_target(target_name,all_targets):\n",
    "    for i in range(0,len(all_targets)):\n",
    "        if all_targets[i] == target_name:\n",
    "            found_idx = i\n",
    "            return found_idx\n",
    "\n",
    "target_lists = [['microsoft','ms_outlook','ms_office','ms_bing','ms_onedrive','ms_skype'],['apple','itunes','icloud'],['google','google_drive'],['alibaba','aliexpress']]\n",
    "\n",
    "def get_associated_targets_idx(target_lists,all_targets):\n",
    "    sub_target_lists_idx = []\n",
    "    parents_ids = []\n",
    "    for i in range(0,len(target_lists)):\n",
    "        target_list = target_lists[i]\n",
    "        parent_target = target_list[0]\n",
    "        one_target_list = []\n",
    "        parent_idx = get_idx_of_target(parent_target,all_targets)\n",
    "        parents_ids.append(parent_idx)\n",
    "        for child_target in target_list[1:]:\n",
    "            child_idx = get_idx_of_target(child_target,all_targets)\n",
    "            one_target_list.append(child_idx)\n",
    "        sub_target_lists_idx.append(one_target_list)\n",
    "    return parents_ids,sub_target_lists_idx \n",
    "\n",
    "parents_ids,sub_target_lists_idx  = get_associated_targets_idx(target_lists,all_targets)\n",
    "\n",
    "def check_if_same_category(img_label1,img_label2):\n",
    "    if_same = 0\n",
    "    if img_label1 in parents_ids:\n",
    "        if img_label2 in sub_target_lists_idx[parents_ids.index(img_label1)]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[0]:\n",
    "        if img_label2 in sub_target_lists_idx[0] or img_label2 == parents_ids[0]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[1]:\n",
    "        if img_label2 in sub_target_lists_idx[1] or img_label2 == parents_ids[1]:\n",
    "            if_same = 1\n",
    "    elif img_label1 in sub_target_lists_idx[2]:\n",
    "        if img_label2 in sub_target_lists_idx[2] or img_label2 == parents_ids[2]:\n",
    "            if_same = 1\n",
    "    return if_same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample triplets (of normal data)\n",
    "def get_batch(batch_size,num_targets):\n",
    "   \n",
    "    # initialize 3 empty arrays for the input image batch\n",
    "    h = X_train_legit.shape[1]\n",
    "    w = X_train_legit.shape[2]\n",
    "    triple=[np.zeros((batch_size, h, w,3)) for i in range(3)]\n",
    "\n",
    "    for i in range(0,batch_size):\n",
    "        img_idx_pair1 = pick_first_img_idx(labels_start_end_train_legit,num_targets)\n",
    "        triple[0][i,:,:,:] = X_train_legit[img_idx_pair1,:]\n",
    "        img_label = int(y_train_legit[img_idx_pair1])\n",
    "        \n",
    "        #get image for the second: positive\n",
    "        triple[1][i,:,:,:] = pick_pos_img_idx(0.15,img_label)\n",
    "            \n",
    "        #get image for the thrid: negative from legit\n",
    "        img_neg,label_neg = pick_neg_img(img_label,num_targets)\n",
    "        while check_if_same_category(img_label,label_neg) == 1:\n",
    "            img_neg,label_neg = pick_neg_img(img_label,num_targets)\n",
    "\n",
    "        triple[2][i,:,:,:] = img_neg\n",
    "          \n",
    "    return triple\n",
    "\n",
    "# Generate adv example for one image \n",
    "def get_adv_example(triple,epsilon,batch_size):\n",
    "    \n",
    "    # Initialize adversarial example \n",
    "    anchor_adv = np.zeros_like(triple[0])\n",
    "    # Added noise\n",
    "    anchor_noise = np.zeros_like(triple[0])\n",
    "\n",
    "    y_true = tf.placeholder(\"float\", [None,1])\n",
    "    target = np.zeros([batch_size,1])\n",
    "    target.astype(float)\n",
    "    \n",
    "    # Get the loss and gradient of the loss wrt the inputs\n",
    "    loss_val = my_loss(y_true, model.output)\n",
    "    grads = K.gradients(loss_val, model.input[0])\n",
    "    \n",
    "    # Get the sign of the gradient\n",
    "    delta = K.sign(grads[0])\n",
    "    \n",
    "    dict_input = {y_true:target,model.input[0]:triple[0],model.input[1]:triple[1],model.input[2]:triple[2] }\n",
    "    delta1 = sess.run(delta, feed_dict=dict_input)\n",
    "    \n",
    "    # Get noise\n",
    "    anchor_noise = anchor_noise + delta1\n",
    "    \n",
    "    # Perturb the image\n",
    "    anchor_adv = triple[0] + epsilon*delta1\n",
    "    \n",
    "    return anchor_noise,anchor_adv\n",
    "\n",
    "# Get batch of adv examples \n",
    "def get_batch_adv(batch_size,num_targets):\n",
    "   \n",
    "    # initialize 3 empty arrays for the input image batch\n",
    "    h = X_train_legit.shape[1]\n",
    "    w = X_train_legit.shape[2]\n",
    "    triple=[np.zeros((batch_size, h, w,3)) for i in range(3)]\n",
    "\n",
    "    for i in range(0,batch_size):\n",
    "        img_idx_pair1 = pick_first_img_idx(labels_start_end_train_legit,num_targets)\n",
    "        triple[0][i,:,:,:] = X_train_legit[img_idx_pair1,:]\n",
    "        img_label = int(y_train_legit[img_idx_pair1])\n",
    "        \n",
    "        #get image for the second: positive\n",
    "        triple[1][i,:,:,:] = pick_pos_img_idx(0.15,img_label)\n",
    "            \n",
    "        #get image for the thrid: negative from legit\n",
    "        img_neg,label_neg = pick_neg_img(img_label,num_targets)\n",
    "        while check_if_same_category(img_label,label_neg) == 1:\n",
    "            img_neg,label_neg = pick_neg_img(img_label,num_targets)\n",
    "\n",
    "        triple[2][i,:,:,:] = img_neg\n",
    "        \n",
    "    epsilon = np.random.uniform(low=0.003, high=0.01) \n",
    "    triple_noise,triple_adv = get_adv_example(triple,epsilon,batch_size)\n",
    "    triple[0] = triple_adv\n",
    "    return triple\n",
    "\n",
    "# Sample two batches (one for adv examples and one for normal images)\n",
    "def get_two_batches(batch_size,num_targets):\n",
    "    half_batch = int(batch_size/2)\n",
    "    triple1 = get_batch(half_batch,num_targets)\n",
    "    triple2 = get_batch_adv(half_batch,num_targets)\n",
    "    \n",
    "    h = X_train_legit.shape[1]\n",
    "    w = X_train_legit.shape[2]\n",
    "    triple =  [np.zeros((batch_size, h, w,3)) for i in range(3)]\n",
    "    \n",
    "    triple[0][0:half_batch,:] = triple1[0]\n",
    "    triple[1][0:half_batch,:] = triple1[1]\n",
    "    triple[2][0:half_batch,:] = triple1[2]\n",
    "\n",
    "    triple[0][half_batch:batch_size,:] = triple2[0]\n",
    "    triple[1][half_batch:batch_size,:] = triple2[1]\n",
    "    triple[2][half_batch:batch_size,:] = triple2[2]\n",
    "    \n",
    "    return triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keras_model(model):\n",
    "    model.save(output_dir+new_saved_model_name+'.h5')\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "targets_train = np.zeros([batch_size,1])\n",
    "for i in range(1, n_iter):\n",
    "    inputs=get_two_batches(batch_size,num_targets)\n",
    "    loss_value=model.train_on_batch(inputs,targets_train)\n",
    "    \n",
    "    print(\"\\n ------------- \\n\")\n",
    "    print('Iteration: '+ str(i) +'. '+ \"Loss: {0}\".format(loss_value))\n",
    "    \n",
    "    if i % save_interval == 0:\n",
    "        save_keras_model(model)\n",
    "        \n",
    "    if i%lr_interval ==0:\n",
    "        start_lr = 0.99*start_lr\n",
    "        K.set_value(model.optimizer.lr, start_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_model = model.layers[3]\n",
    "\n",
    "whitelist_emb = shared_model.predict(X_train_legit,batch_size=64)\n",
    "np.save(output_dir+'whitelist_emb_adv',whitelist_emb)\n",
    "np.save(output_dir+'whitelist_labels_adv',y_train_legit )\n",
    "\n",
    "phishing_emb = shared_model.predict(all_imgs_test,batch_size=64)\n",
    "np.save(output_dir+'phishing_emb_adv',phishing_emb)\n",
    "np.save(output_dir+'phishing_labels_adv',all_labels_test )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
